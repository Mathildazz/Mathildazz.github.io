<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于KNN的一些学习笔记]]></title>
    <url>%2F2017%2F10%2F28%2F%E5%85%B3%E4%BA%8EKNN%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[What is the KNN? 在模式识别领域，最近邻居法（KNN算法）是一种用于分类(分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类。)和回归（一种统计学上分析数据的方法，目的在于了解两个或多个变数间是否相关、相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。）的非参数统计方法（适用于母群体分布情况未明、小样本、母群体分布不为常态也不易转换为常态。特点在于尽量减少或不修改其建立之模型，较具稳健特性。）在这两种情况下，输入包含特征空间中的k个最接近的训练样本。在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若k = 1，则该对象的类别直接由最近的一个节点赋予。 KNN算法是用来做分类的，换句话说，有一个样本空间里的样本分成几个类型，然给定一个待分类的数据，通过计算距离自己最近的K个样本来判断这个待分类的样本属于哪一类。即由离自己最近的那K个点投票决定待分类数据的类别。 如上图所示，图中有两种类型的样本数据，一类是蓝色的正方形，一类是红色的三角形，而中间的绿色圆形则是待分类的数据。 如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，由这3个点投票，于是绿色的这个待分类点属于红色的三角形。如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，由这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。 KNN的基本思想可以简单的理解为：由那几个离自己最近的K个点来投票决定待分类的数据归为哪一类。 KNN的实现步骤： 1.计算距离：计算样本中所有数据点与未知数据点的距离； 2.找到邻居：将距离未知数据点最近的K个样本数据点作为该未知数据的“邻居”； 3.归类：根据这K个“邻居”的类别投票决定未知数据的类别。 PS：这里的距离衡量包括欧氏距离或者夹角余弦等，就文本分类而言，使用余弦来计算相似度就比欧氏距离更加合适。 关于KNN的优缺点总结如下： 1.优点：易于理解和实现，无需估计参数，无需训练。适用于对稀有事件进行分类，特别适合于多分类问题（对象具有多个类别的标签）。 2.缺点：分类速度慢、各属性权重相同的情况下影响了准确率、对样本库容量的依赖性较强以及K值不好确定。 一些改进措施 １.降低计算复杂度 （1）删除一些对分类结果影响较小的属性 （2）缩小训练样本 （3）通过聚类，将其得到的中心点作为新的训练样本 ２.优化相似度度量方法：给特征赋予不同的权重 这个地方可以参考加权KNN ３.优化判决策略可采用均匀化样本分布密度的方法 ４.选取恰当的K值，目前只能通过反复的试验调整选取比较好的K值。 小结KNN算法简单，易于实现，但是当样本规模很大的时候，其复杂度会很大，所谓“适合的才是最好的”，因此在选择分类算法的时候我们应该根据具体应用的需求，选择适当的分类算法。 The End]]></content>
  </entry>
  <entry>
    <title><![CDATA[This is my FirstBlog]]></title>
    <url>%2F2017%2F10%2F26%2FThis-is-my-FirstBlog%2F</url>
    <content type="text"><![CDATA[I hope this is a good start! Being cute and great~~~~]]></content>
  </entry>
</search>
